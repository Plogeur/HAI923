{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8wXi6kgR7AFWzgGfT9/UL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Plogeur/HAI923/blob/main/Notebook_GAN%26AE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> Notebook : Création des modèles GAN, AE et VAE </H1>\n",
        "\n",
        "Bienvenue dans ce notebook dédié à l'exploration en profondeur des générateurs adverses (GANs), des autoencodeurs (AEs) et des autoencodeurs variationnels (VAEs). Les GANs, conceptualisés par Ian Goodfellow et ses collègues en 2014, ont révolutionné la génération de données en introduisant un cadre d'entraînement basé sur la compétition entre un générateur et un discriminateur. Ils ont permis de créer des données synthétiques d'une qualité inégalée, propulsant ainsi des avancées majeures dans des domaines tels que la génération d'images photoréalistes et la synthèse de vidéos. Les autoencodeurs (AE), quant à eux, ont ouvert la voie à des techniques de compression de données innovantes en apprenant à représenter de manière compacte l'information essentielle d'une donnée tout en minimisant la perte d'information. Ils ont été largement utilisés pour des tâches de débruitage, de reconstruction, et de réduction de dimensionnalité. Les autoencodeurs variationnels (VAEs) ont apporté une dimension probabiliste à l'apprentissage de représentations. Ils ont la capacité unique de générer de nouvelles données en explorant un espace latent continu. Cette approche probabiliste a ouvert de nouvelles perspectives dans la génération de données avec une contrôlabilité fine.\n",
        "\n",
        "Dans ce notebook avancé, nous plongerons profondément dans ces trois paradigmes, explorant leurs mécanismes internes, leurs architectures avancées, ainsi que leurs applications de pointe. Nous discuterons également des défis et des limites actuelles de ces techniques et des pistes de recherche futures.\n",
        "\n",
        "*Note à moi-même : putain j'ai pas envie de le faire celui-la, ca va être longue...*"
      ],
      "metadata": {
        "id": "wIRgpWtYmyQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yaxingwang/MineGAN/tree/master/styleGANv2"
      ],
      "metadata": {
        "id": "7Ms8s8HHsf0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yaxingwang/Transferring-GANs"
      ],
      "metadata": {
        "id": "KW0R6NkODfbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/mshahbazi72/cGANTransfer"
      ],
      "metadata": {
        "id": "IwoRbuwcDfea"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQSC8frX5Ao"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import random\n",
        "import shutil\n",
        "import keras\n",
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import cv2\n",
        "from keras import backend as K\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "from keras import metrics\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.saving import load_model\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dropout, Dense, GlobalAveragePooling2D, Flatten, Rescaling, Conv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose, LeakyReLU\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL VARIABLE\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 8\n",
        "CHANEL = 3\n",
        "N_KFOLDS = 10\n",
        "STOPPING_PATIENCE = 15\n",
        "REDUCTION_PATIENCE = 5\n",
        "EPOCHS = 500\n",
        "VERBOSE = 1\n",
        "COLUMNS = 25\n",
        "SEED = 123\n",
        "NUMBERCLASS = 3\n",
        "PATH_DIR = \"/content/gdrive/MyDrive/Colab_Notebooks/HAI923/Tiger-Fox-Elephant/\"\n",
        "\n",
        "# SET SEED\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "\n",
        "# CALLBACKS\n",
        "def callbacks(modelName) :\n",
        "  EARLY_STOPPING = \\\n",
        "          EarlyStopping(\n",
        "              monitor='val_loss',\n",
        "              patience=STOPPING_PATIENCE,\n",
        "              verbose=VERBOSE,\n",
        "              mode='auto')\n",
        "\n",
        "  LR_REDUCTION = \\\n",
        "          ReduceLROnPlateau(\n",
        "              monitor='val_accuracy',\n",
        "              patience=REDUCTION_PATIENCE,\n",
        "              verbose=VERBOSE,\n",
        "              factor=0.2,\n",
        "              min_lr=0.000001)\n",
        "\n",
        "  CHECKPOINT = ModelCheckpoint(f\"{modelName}.h5\", monitor='val_accuracy', verbose=VERBOSE,\n",
        "      save_best_only=True, mode='auto', save_freq=\"epoch\")\n",
        "\n",
        "  CALLBACKS = [EARLY_STOPPING, LR_REDUCTION, CHECKPOINT]\n",
        "  return CALLBACKS\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.F1Score(threshold=0.5, average='macro', name='f1_score')\n",
        "      ]\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd /content/gdrive/MyDrive/Colab_Notebooks/HAI923/\n",
        "path = %pwd"
      ],
      "metadata": {
        "id": "7c2nI_bUG0lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a30005b-01ad-41ad-90c5-3b348009e86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/Colab_Notebooks/HAI923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "2EbSJ_0DGdYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rgb(my_path, my_classes) :\n",
        "  X,y=create_X_y(my_path, my_classes)\n",
        "  print(\"Les classes : \", my_classes)\n",
        "  print(\"Nombres de données : \", X.shape[0])\n",
        "  print (\"Résolution des images : \", X[0].shape)\n",
        "  X=X.astype('float')\n",
        "  X=X/255.0\n",
        "  return X,y\n",
        "\n",
        "def create_training_data(path_data, list_classes):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "    path=os.path.join(path_data, classes)\n",
        "    class_num=list_classes.index(classes)\n",
        "    for img in os.listdir(path):\n",
        "      try :\n",
        "        img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_UNCHANGED)\n",
        "        new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        training_data.append([new_array, class_num])\n",
        "      except Exception as e:\n",
        "        pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y (path_data, list_classes):\n",
        "      training_data=create_training_data(path_data, list_classes)\n",
        "      random.shuffle(training_data)\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      #y=to_categorical(y) #onehot\n",
        "      y=np.array(y)\n",
        "\n",
        "      return X,y"
      ],
      "metadata": {
        "id": "CNOaTxn3Slml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "5A9Q5uksoYR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQUwrbQeFF1F"
      },
      "outputs": [],
      "source": [
        "def create_generator(latent_dim=100) :\n",
        "  epsilon = 0.0001 # Small float added to variance to avoid dividing by zero in the BatchNorm layers.\n",
        "  noise_shape = (100,)\n",
        "  size = int(IMG_SIZE/(32*2))\n",
        "  model = Sequential()\n",
        "\n",
        "  # Définition des couches de Convolution/Pooling\n",
        "  model.add(Dense(size*size*512*2, activation='linear', input_shape=noise_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((size, size, 512*2)))\n",
        "\n",
        "  model.add(Conv2DTranspose(512*2, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(256*2, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(128*2, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(64*2, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(64, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2DTranspose(32, kernel_size=[4,4], strides=[2,2], padding=\"same\", kernel_initializer= keras.initializers.TruncatedNormal(stddev=0.02)))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT3RJNkuFHpt"
      },
      "outputs": [],
      "source": [
        "def create_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  # Définition des couches de Convolution/Pooling\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same', input_shape=[IMG_SIZE,IMG_SIZE,3]))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  model.add(Conv2D(2*128, (3,3), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(MaxPooling2D(pool_size=(3,3)))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  # Flattening : passage de matrices 3D vers un vecteur\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(2*128))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  # Couche de sortie : classification => binaire : 1 -> vraie image, 0 -> fausse image\n",
        "  model.add(Dense(2*128))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # compilation du  modèle de classification\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzmkF8YvGGHp"
      },
      "outputs": [],
      "source": [
        "# Chargement et normalisation des données\n",
        "def load_data(animal):\n",
        "  my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n",
        "  my_classes=[animal]\n",
        "  X,y=create_X_y(my_path,my_classes)\n",
        "  # Surtout ne pas oublier de normaliser les données avec :\n",
        "  X=X.astype('float32')\n",
        "  #X=X/255.0\n",
        "  X=(X - 127.5) / 127.5\n",
        "  return X\n",
        "\n",
        "# Creation d'un jeu de données de vraies images\n",
        "# les vraies images sont labélisées avec 1\n",
        "def generate_real_samples(dataset, nb_images):\n",
        "  # tirage aléatoire\n",
        "  ix = randint(0, dataset.shape[0], nb_images)\n",
        "  #serie = random.sample(range(0, dataset.shape[0]), nb_images)\n",
        "  # sélection des images\n",
        "  X = dataset[ix]\n",
        "  # mettre 1 comme label de classe\n",
        "  y = ones((nb_images, 1))\n",
        "  return X, y\n",
        "\n",
        "# Création d'un faux jeu de données\n",
        "# elles seront labélisées avec 0\n",
        "def generate_fake_samples(nb_images):\n",
        "  X = np.random.rand(IMG_SIZE * IMG_SIZE * 3 * nb_images)\n",
        "  # reshape en images grises\n",
        "  X = X.reshape((nb_images, IMG_SIZE, IMG_SIZE, 3))\n",
        "  # mettre 0 comme label de classe\n",
        "  y = zeros((nb_images, 1))\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzDCJxGGGKcw"
      },
      "outputs": [],
      "source": [
        "# train the discriminator model\n",
        "def train_discriminator(model, dataset, epochs=100, batchsize=20):\n",
        "  # on constitue un jeu de données de batchsize/2 images réelles et images fausses\n",
        "\thalf_batch = int(batchsize / 2)\n",
        "\t# boucler sur les epochs\n",
        "\tfor i in range(epochs):\n",
        "\t\t# sélection d'images réelles\n",
        "\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t# mettre à jour le discriminateur avec les images réelles\n",
        "\t\t_, real_acc = model.train_on_batch(X_real, y_real)\n",
        "\t\t# generation de fausses images\n",
        "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
        "\t\t# mise à jour du discriminateur avec de fausses images\n",
        "\t\t_, fake_acc = model.train_on_batch(X_fake, y_fake)\n",
        "\t\t# Affichage des résultats pour l'accuracy des vraies et des fausses\n",
        "\t\tprint('>%d accuracy_real=%.0f%% accuracy_fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boRjKhueGLuZ"
      },
      "outputs": [],
      "source": [
        "def create_gan(generator_model, discriminator_model):\n",
        "  # mettre les poids du discriminateur non entrable\n",
        "  discriminator_model.trainable = False\n",
        "  # création d'un seul modele qui regroupe generateur et discriminateur\n",
        "  model = Sequential()\n",
        "  # ajout du générateur\n",
        "  model.add(generator_model)\n",
        "  # ajout du discriminateur\n",
        "  model.add(discriminator_model)\n",
        "\n",
        "  opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWw1nXmCGNhQ"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim, nb_images):\n",
        "  X_input = np.random.randn(latent_dim * nb_images)\n",
        "  X_input = X_input.reshape(nb_images, latent_dim)\n",
        "  return X_input\n",
        "\n",
        "def generate_fake_samples_for_generator(generator_model, latent_dim, nb_images):\n",
        "  # generation des points\n",
        "  X_input = generate_latent_points (latent_dim,nb_images)\n",
        "  # prediction de la sortie du générateur\n",
        "  X = generator_model.predict(X_input)\n",
        "  # A ce niveau on considère que les images sont fausses\n",
        "  # donc on met 0 comme label.\n",
        "  y = zeros((nb_images, 1))\n",
        "  return X, y\n",
        "\n",
        "def plot_and_save_generatedimages(generated_images, epoch, nb_images=10):\n",
        "\t# Affichage des images\n",
        "\tfor i in range(nb_images * nb_images):\n",
        "\t\tpyplot.subplot(nb_images, nb_images, 1 + i)\n",
        "\t\tpyplot.axis('off')\n",
        "\t\tpyplot.imshow(generated_images[i, :, :, 0], cmap='gray_r')\n",
        "\t# sauvegarde de l'image\n",
        "\tfilename = 'generated_plot_FashionCNN_withGan%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(filename)\n",
        "\tpyplot.close()\n",
        "\n",
        "def evaluate_model(dataset, epoch, generator_model, discriminator_model, latent_dim, nb_images=100, save_model=False):\n",
        "  # récupération de vraies images pour le discriminateur\n",
        "  X_real, y_real = generate_real_samples(dataset, nb_images)\n",
        "  # Evaluation de l'accuracy pour le discriminateur\n",
        "  _, acc_real = discriminator_model.evaluate(X_real, y_real, verbose=0)\n",
        "\n",
        "  # génération de fausses images pour le générateur et donc le gan\n",
        "  X_fake, y_fake = generate_fake_samples_for_generator(generator_model, latent_dim, nb_images)\n",
        "  # Evaluation du discriminateur avec des fausses images\n",
        "  _, acc_fake = discriminator_model.evaluate(X_fake, y_fake, verbose=0)\n",
        "  print ('Accuracy reélle : %.0f%%, fausse : %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\n",
        "  if save_model==True:\n",
        "    # sauvegarde du generateur pour un autre usage\n",
        "    filename = 'generator_model_GAN%03d.h5' % (epoch + 1)\n",
        "    generator_model.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5bVd0dbGPZK"
      },
      "outputs": [],
      "source": [
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=25):\n",
        "  # Pour déterminer combien il y aura de batchs analysés à chaque epoch\n",
        "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "\t# manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "    print(\"epoche n°\",i)\n",
        "    for j in range(bat_per_epo):\n",
        "\t\t\t  # get randomly selected 'real' samples\n",
        "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "        # update discriminator model weights\n",
        "        d_loss1, _ = d_model.train_on_batch(X_real, y_real, reset_metrics=False)\n",
        "        # generate 'fake' examples\n",
        "        X_fake, y_fake = generate_fake_samples_for_generator(g_model, latent_dim, half_batch)\n",
        "        # update discriminator model weights\n",
        "        d_loss2, _ = d_model.train_on_batch(X_fake, y_fake, reset_metrics=False)\n",
        "        # prepare points in latent space as input for the generator\n",
        "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "        # create inverted labels for the fake samples\n",
        "        y_gan = ones((n_batch, 1))\n",
        "        # update the generator via the discriminator's error\n",
        "        g_loss = gan_model.train_on_batch(X_gan, y_gan, reset_metrics=False)\n",
        "\n",
        "    #affiche tout les 20 epochs les images générés\n",
        "    if i % 10 == 0 and i > 0 :\n",
        "      print('>%d, %d/%d, dis_loss_true=%.3f, dis_loss_false=%.3f gan_loss=%.3f' %\n",
        "      (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "      evaluate_model (dataset, i, generator_model, discriminator_model, latent_dim)\n",
        "      latent_dim=100\n",
        "      COLUMNS = 25\n",
        "      plt.figure(figsize=(15,15))\n",
        "\n",
        "      for i in range(COLUMNS):\n",
        "        plt.subplot(5,5,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        noise=tf.random.normal([1,latent_dim])\n",
        "        plt.imshow((generator_model.predict(noise)[0]+1)/2)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TL model"
      ],
      "metadata": {
        "id": "CIVFPtHfpXlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mshahbazi72/cGANTransfer.git"
      ],
      "metadata": {
        "id": "OEElKneVVfBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "De3-8PejbsKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcQGRWB-bsIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxGP0gFvbsGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPQQCQfTbsEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}