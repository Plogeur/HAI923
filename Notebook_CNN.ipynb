{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Plogeur/HAI923/blob/master/Notebook_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHoI_qwJETpf"
      },
      "source": [
        "<H1> Notebook : Création des modèles CNN </H1>\n",
        "\n",
        "Le modèles de réseaux de neurones convolutionnels Convolutional Neural Networks, sont devenus l'épine dorsale de nombreuses applications de pointe en traitement d'images, de la détection d'objets à la segmentation sémantique en passant par la reconnaissance d'images. Au cours de ce notebook, nous aborderons des sujets tels que l'architecture de base d'un CNN, la sélection de couches de convolution, de pooling et de normalisation, ainsi que les stratégies de régularisation pour améliorer la généralisation. Nous explorerons également le transfert d'apprentissage et le fine tuning à l'aide de réseaux pré-entraînés, afin d'exploiter des modèles déjà entraînés pour résoudre des problèmes spécifiques. Enfin, à moins que la procrastination ne l'emporte, nous aborderons également les transformateurs avec l'utilisation de l'architecture ViT.\n",
        "\n",
        "*Note à moi-même : c'est mort pour le VIT j'ai déjà la flemme de faire le reste*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VQSC8frX5Ao"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7eBBrtYb7_z",
        "outputId": "d274d24d-6d2e-4689-e5bf-b42943674da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.10/dist-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.23.5)\n",
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-self-attention\n",
        "!pip install vit-keras\n",
        "!pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZw2C6r8YJWW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import random\n",
        "import shutil\n",
        "import keras\n",
        "import pathlib\n",
        "import sys\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "from keras import metrics\n",
        "from scipy import stats\n",
        "from vit_keras import vit, utils\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LSTM\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.saving import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, Adamax, Lion\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Input, Activation, Reshape, Dropout, Dense, GlobalAveragePooling2D, Flatten, Rescaling, Conv2D, BatchNormalization, MaxPooling2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c2nI_bUG0lo",
        "outputId": "86ea3b89-8459-4c71-d35c-0bbff51a5ac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/Colab Notebooks\n",
            "/content/gdrive/MyDrive/Colab Notebooks/HAI923\n"
          ]
        }
      ],
      "source": [
        "# GLOBAL VARIABLE\n",
        "IMG_SIZE = 256\n",
        "CHANEL = 3\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, CHANEL)\n",
        "BATCH_SIZE = 4\n",
        "N_KFOLDS = 10\n",
        "STOPPING_PATIENCE = 200\n",
        "REDUCTION_PATIENCE = 15\n",
        "EPOCHS = 250\n",
        "VERBOSE = 1\n",
        "COLUMNS = 25\n",
        "SEED = 123\n",
        "POLICE_SIZE = 18 # Taille de la police pour les plot\n",
        "plt.rcParams.update({'font.size': POLICE_SIZE})\n",
        "\n",
        "# SET SEED\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# CALLBACKS\n",
        "def callbacks(modelName) :\n",
        "  EARLY_STOPPING = \\\n",
        "          EarlyStopping(\n",
        "              monitor='val_loss',\n",
        "              patience=STOPPING_PATIENCE,\n",
        "              verbose=VERBOSE,\n",
        "              mode='auto')\n",
        "\n",
        "  LR_REDUCTION = \\\n",
        "          ReduceLROnPlateau(\n",
        "              monitor='val_accuracy',\n",
        "              patience=REDUCTION_PATIENCE,\n",
        "              verbose=VERBOSE,\n",
        "              factor=0.5,\n",
        "              min_lr=0.00001)\n",
        "\n",
        "  CHECKPOINT = ModelCheckpoint(f\"Saved_Model/{modelName}.h5\", monitor='val_accuracy', verbose=VERBOSE,\n",
        "      save_best_only=True, mode='auto', save_freq=\"epoch\")\n",
        "\n",
        "  CALLBACKS = [EARLY_STOPPING, LR_REDUCTION, CHECKPOINT]\n",
        "  return CALLBACKS\n",
        "\n",
        "METRICS = [\n",
        "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      tf.keras.metrics.Precision(name='precision'),\n",
        "      tf.keras.metrics.Recall(name='recall'),\n",
        "      tf.keras.metrics.F1Score(threshold=0.5, dtype='float32', name='f1_score')\n",
        "      ]\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd /content/gdrive/MyDrive/Colab Notebooks/\n",
        "%mkdir -p HAI923/Résultats/Entrainement/\n",
        "%mkdir -p HAI923/Saved_Model/\n",
        "%cd HAI923/\n",
        "%pwd\n",
        "\n",
        "import zipfile\n",
        "if not(os.path.exists('Tiger-Fox-Elephant/')) :\n",
        "  if not(os.path.exists('Tiger-Fox-Elephant.zip')) :\n",
        "    !wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip\n",
        "  with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref :\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EbSJ_0DGdYH"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaHyQ2mY15Ie"
      },
      "outputs": [],
      "source": [
        "def create_dataset(my_path, my_classes, gray=False, onehot=False) :\n",
        "  X,y=create_X_y(my_path, my_classes, gray, onehot)\n",
        "  print(\"Les classes : \", my_classes)\n",
        "  print(\"Nombres de données : \", X.shape[0])\n",
        "  print (\"Résolution des images : \", X[0].shape)\n",
        "  X=X.astype('float')\n",
        "  X=X/255.0\n",
        "  return X,y\n",
        "\n",
        "def create_training_data(path_data, list_classes, gray=False):\n",
        "  training_data=[]\n",
        "  for classes in list_classes:\n",
        "    path=os.path.join(path_data, classes)\n",
        "    class_num=list_classes.index(classes)\n",
        "    for img in os.listdir(path):\n",
        "      try :\n",
        "        if gray == False :\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.COLOR_BGR2RGB)\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "        else :\n",
        "          img_array = cv2.imread(os.path.join(path,img), cv2.COLOR_BGR2GRAY)\n",
        "          equ = cv2.equalizeHist(new_array)\n",
        "          new_array = np.hstack((new_array,equ))\n",
        "        training_data.append([new_array, class_num])\n",
        "      except Exception as e:\n",
        "        pass\n",
        "  return training_data\n",
        "\n",
        "def create_X_y(path_data, list_classes, gray=False, onehot=False):\n",
        "      training_data=create_training_data(path_data, list_classes, gray)\n",
        "      random.shuffle(training_data)\n",
        "      X=[]\n",
        "      y=[]\n",
        "      for features, label in training_data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "      if gray == False :\n",
        "        X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n",
        "      else :\n",
        "        X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 1)\n",
        "      if onehot == False :\n",
        "        y=np.array(y, dtype=np.float32)\n",
        "      else :\n",
        "        y=to_categorical(y, dtype=np.float32) #onehot\n",
        "\n",
        "      return X,y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrainement"
      ],
      "metadata": {
        "id": "vaeN26nLNAJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcd9vnFHcf6x"
      },
      "outputs": [],
      "source": [
        "def trainModelNumpyAugment(givenModel, modelName=None) :\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        shear_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    if modelName == None :\n",
        "      model = givenModel()\n",
        "      CALLBACKS = callbacks(str(givenModel.__name__))\n",
        "    else :\n",
        "      model = givenModel(modelName)\n",
        "      CALLBACKS = callbacks(str(givenModel.__name__)+str(modelName.__name__))\n",
        "\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # fit du modele\n",
        "    history = model.fit(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
        "                        validation_data=(X_val, Y_val), epochs=EPOCHS,\n",
        "                        verbose=VERBOSE, callbacks=CALLBACKS)\n",
        "\n",
        "    # evaluate du modele\n",
        "    model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "\n",
        "    #plot acc and loss in function of epochs\n",
        "    if modelName == None :\n",
        "      plotCurve(history, str(givenModel.__name__))\n",
        "    else :\n",
        "      plotCurve(history, str(givenModel.__name__)+str(modelName.__name__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mediane(liste):\n",
        "    liste_triee = sorted(liste)\n",
        "    n = len(liste_triee)\n",
        "\n",
        "    if n % 2 == 1:\n",
        "        # Cas de nombre impair d'éléments\n",
        "        mediane = liste_triee[n // 2]\n",
        "    else:\n",
        "        # Cas de nombre pair d'éléments\n",
        "        milieu1 = liste_triee[n // 2 - 1]\n",
        "        milieu2 = liste_triee[n // 2]\n",
        "        mediane = (milieu1 + milieu2) / 2\n",
        "\n",
        "    return mediane"
      ],
      "metadata": {
        "id": "EPQ96m45NwVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s0y5Az1G4PD"
      },
      "outputs": [],
      "source": [
        "def trainModelNumpyKfoldAugment(givenModel, modelName=None) :\n",
        "    scores, histories = [[],[],[],[],[]], list()\n",
        "\n",
        "    kfold = KFold(N_KFOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        shear_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    if modelName == None :\n",
        "      CALLBACKS = callbacks(givenModel.__name__)\n",
        "    else :\n",
        "      CALLBACKS = callbacks(str(givenModel.__name__)+str(modelName.__name__))\n",
        "\n",
        "    # parcourir les splits du k-fold\n",
        "    for fold, (train_ix, eval_ix) in enumerate(kfold.split(X_train), start=1):  # Add enumerate to track the fold number\n",
        "\n",
        "      print(\"\")\n",
        "      print(f\"############# Fold n°{fold} #############\")\n",
        "\n",
        "      # Redéfinition du modèle\n",
        "      if modelName == None :\n",
        "        model = givenModel()\n",
        "      else :\n",
        "        model = givenModel(modelName)\n",
        "\n",
        "      split_point = int(len(train_ix) * 0.920)\n",
        "\n",
        "      # Combine eval_ix elements with train_ix for validation data\n",
        "      X_val = np.concatenate((X_train[train_ix[split_point:]], X_train[eval_ix]), axis=0)\n",
        "      y_val = np.concatenate((Y_train[train_ix[split_point:]], Y_train[eval_ix]), axis=0)\n",
        "\n",
        "      # Selection des données pour training\n",
        "      x_train, y_train = X_train[train_ix[:split_point]], Y_train[train_ix[:split_point]]\n",
        "\n",
        "      print(f\"len(y_train) : {len(y_train)} and len(y_val) : {len(y_val)}\")\n",
        "\n",
        "      datagen.fit(x_train)\n",
        "\n",
        "      # fit du modele\n",
        "      history = model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                          validation_data=(X_val, y_val), epochs=EPOCHS,\n",
        "                          verbose=VERBOSE, callbacks=CALLBACKS)\n",
        "\n",
        "      # evaluate du modele\n",
        "      loss, acc, pre, rec, fscore = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "\n",
        "      # Create a list of values from the evaluation\n",
        "      evaluation_values = [loss, acc, pre, rec, fscore]\n",
        "\n",
        "      # Append the values to the respective scores list\n",
        "      for i, value in enumerate(evaluation_values):\n",
        "          scores[i].append(value)\n",
        "\n",
        "      histories.append(history)\n",
        "\n",
        "    #plot acc and loss in function of epochs\n",
        "    if modelName == None :\n",
        "      plotCurvesKfolding(histories, str(givenModel.__name__))\n",
        "    else :\n",
        "      plotCurvesKfolding(histories, str(givenModel.__name__)+str(modelName.__name__))\n",
        "\n",
        "    ListMetrics = [\"loss\", \"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n",
        "\n",
        "    for metric, ListMetricValue in zip(ListMetrics, scores) :\n",
        "\n",
        "      # Calculate mean and standard deviation\n",
        "      mean_metric = np.mean(ListMetricValue)\n",
        "      std_deviation = np.std(ListMetricValue)\n",
        "\n",
        "      # Calculate the standard error of the mean (SEM)\n",
        "      sem = std_deviation / np.sqrt(len(ListMetricValue))\n",
        "\n",
        "      # Set the desired confidence level (e.g., 95%)\n",
        "      confidence_level = 0.95\n",
        "\n",
        "      # Calculate the margin of error based on the confidence level\n",
        "      margin_of_error = stats.t.ppf((1 + confidence_level) / 2, len(ListMetricValue) - 1) * sem\n",
        "\n",
        "      # Calculate the confidence interval\n",
        "      lower_bound = mean_metric - margin_of_error\n",
        "      upper_bound = mean_metric + margin_of_error\n",
        "\n",
        "      med_metric = mediane(ListMetricValue)\n",
        "      diff_max_min = (np.max(ListMetricValue) - np.min(ListMetricValue))/2\n",
        "\n",
        "      # Print the results\n",
        "      print(f\"Test {metric} mean ± margin : {mean_metric:.3f} ± {margin_of_error:.3f} (95% CI: {lower_bound:.3f}, {upper_bound:.3f})\")\n",
        "      print(f\"Test {metric} mean ± std : {med_metric:.3f} ± {std_deviation:.3f}\")\n",
        "      print(f\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkvMr3BojJi9"
      },
      "source": [
        "# Visualisation Resultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oSLsGPLrIKv"
      },
      "outputs": [],
      "source": [
        "def plotCurve(history, ModelName) :\n",
        "\n",
        "  #plot acc\n",
        "  plt.figure(1, figsize = (15,8))\n",
        "  plt.subplot(221)\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "  plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  # plot loss\n",
        "  plt.subplot(222)\n",
        "  plt.title('Binary Entropy Loss')\n",
        "  plt.plot(history.history['loss'], color='blue', label='train')\n",
        "  plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f\"Résultats/Entrainement/Accuracy_and_loss_{ModelName}.png\")\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26nB8myL2ObE"
      },
      "outputs": [],
      "source": [
        "def plotCurvesKfolding(histories, ModelName) :\n",
        "\n",
        "  for i in range(len(histories)) :\n",
        "    #plot acc\n",
        "    plt.figure(1, figsize = (15,8))\n",
        "    plt.subplot(221)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(histories[i].history['accuracy'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_accuracy'], color='red', label='test')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "    # plot loss\n",
        "    plt.subplot(222)\n",
        "    plt.title('Binary Entropy Loss') #CHANGE TO CROSS ENTROPY LOSS IF NUM_CLASS > 1\n",
        "    plt.plot(histories[i].history['loss'], color='blue', label='train')\n",
        "    plt.plot(histories[i].history['val_loss'], color='red', label='test')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f\"Résultats/Entrainement/Kfold_Accuracy_and_loss_{ModelName}.png\")\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J86aNzDojI7Z"
      },
      "outputs": [],
      "source": [
        "def plotCurvesFineTuningKfold(historiess, ModelName, next_epochs) :\n",
        "\n",
        "  for histories in historiess :\n",
        "    for i in range(len(histories)) :\n",
        "\n",
        "      number_of_training_session = len(histories[i])\n",
        "      acc = histories[i][0].history['accuracy']\n",
        "      val_acc = histories[i][0].history['val_accuracy']\n",
        "\n",
        "      loss = histories[i][0].history['loss']\n",
        "      val_loss = histories[i][0].history['val_loss']\n",
        "\n",
        "      for j in range(1, len(histories)) :\n",
        "        acc += histories[i][j].history['accuracy']\n",
        "        val_acc += histories[i][j].history['val_accuracy']\n",
        "\n",
        "        loss += histories[i][j].history['loss']\n",
        "        val_loss += histories[i][j].history['val_loss']\n",
        "\n",
        "      plt.figure(figsize=(8, 8))\n",
        "      plt.subplot(2, 1, 1)\n",
        "      plt.plot(acc, label='Training Accuracy')\n",
        "      plt.plot(val_acc, label='Validation Accuracy')\n",
        "      plt.ylim([0.4, 1])\n",
        "\n",
        "      number=1\n",
        "      for epochs in next_epochs :\n",
        "        plt.plot([epochs+1, epochs+1], plt.ylim(), label='Fine Tuning ' + str(number))\n",
        "        number=+1\n",
        "\n",
        "      plt.legend(loc='upper left')\n",
        "      plt.title('Training and Validation Accuracy')\n",
        "\n",
        "      plt.subplot(2, 1, 2)\n",
        "      plt.plot(loss, label='Training Loss')\n",
        "      plt.plot(val_loss, label='Validation Loss')\n",
        "      plt.ylim([0, 4.0])\n",
        "\n",
        "      number=1\n",
        "      for epochs in next_epochs :\n",
        "        plt.plot([epochs+1, epochs+1], plt.ylim(), label='Fine Tuning ' + str(number))\n",
        "        number=+1\n",
        "\n",
        "      plt.legend(loc='upper left')\n",
        "      plt.title('Training and Validation Loss')\n",
        "      plt.xlabel('epoch')\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f\"Résultats/Entrainement/Kfold_Fine_tuning_Kfold_Accuracy_and_loss_{ModelName}.png\")\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vun5YlwnIC48"
      },
      "source": [
        "# Modèles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYhkZkaBeOdj"
      },
      "source": [
        "bon site web : https://paperswithcode.com/task/image-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocC1yn8CKaz6"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg4s8zyK_AVK"
      },
      "source": [
        "This is the first CNN architecture is a basic baseline model for image classification tasks. You can use it as a starting point and further customize it or adjust the number of layers and neurons to fit your specific problem and dataset. To use this model, you would call the baseline() function to obtain an instance of the model and then compile, train, and evaluate it with your data.\n",
        "\n",
        "This model is composed of 3 feature extraction blocks each one gonna reduce the dimensionality of the data. This reduction can help the model focus on the most important information while discarding less relevant details, which can lead to more efficient and effective learning.\n",
        "\n",
        "And a classification part composed of fully connect neuron (Dense) and regularization (dropout) that disable some % of Dense neuron in the training session. At the end you have the final dense layer that classify the picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "148XTrezIGyI"
      },
      "outputs": [],
      "source": [
        "def baseline(NUM_CLASS=1) :\n",
        "  model = Sequential()\n",
        "\n",
        "  # ---- Conv / Pool N°1\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°2\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°3\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # Flattening\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(128, activation='relu')) #, kernel_regularizer=L1L2(1e-4)))  # Add L1L2 regularization\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(64, activation='relu')) #, kernel_regularizer=L1L2(1e-4)))  # Add L1L2 regularization\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  if NUM_CLASS <= 2 :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"sigmoid\"))\n",
        "  else :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"softmax\"))\n",
        "\n",
        "  # compilation du model de classification\n",
        "  opt = Lion(learning_rate=1e-04)\n",
        "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TLO8404Fyw4"
      },
      "outputs": [],
      "source": [
        "def baselineWithRegularization(NUM_CLASS=1) :\n",
        "  model = Sequential()\n",
        "\n",
        "  # ---- Conv / Pool N°1\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°2\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°3\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # Flattening\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(128, activation='relu', kernel_regularizer=L1L2(1e-4)))  # Add L1L2 regularization\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(64, activation='relu', kernel_regularizer=L1L2(1e-4)))  # Add L1L2 regularization\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  if NUM_CLASS <= 2 :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"sigmoid\"))\n",
        "  else :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"softmax\"))\n",
        "\n",
        "  # compilation du model de classification\n",
        "  opt = Lion(learning_rate=1e-04)\n",
        "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgJXlCHq3r72"
      },
      "source": [
        "## Testing trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWjhpxJp3uNd"
      },
      "outputs": [],
      "source": [
        "def TestingAlreadyTrainedModel(modelName) :\n",
        "\n",
        "  model = modelName(weights='imagenet')\n",
        "  ClassModel = [['tiger'],['African_elephant','Indian_elephant','tusker'],['Arctic_fox','red_fox','coyote']]\n",
        "  ListClass = ['tiger','elephant','fox']\n",
        "  print(f\"model {modelName.__name__} as a resolution input img : \", model.input_shape[1:3])\n",
        "  ClassModelIndex=0\n",
        "\n",
        "  # Loop through each class directory\n",
        "  for class_dir in ListClass :\n",
        "    GoodPred = 0\n",
        "    NbimgClass = 0\n",
        "    averagePred = []\n",
        "    confusionClass = []\n",
        "    misclassifications = {}\n",
        "\n",
        "    # Loop through the images in the class directory\n",
        "    for img_filename in os.listdir('Tiger-Fox-Elephant/'+class_dir) :\n",
        "      img_path = os.path.join('Tiger-Fox-Elephant/'+class_dir, img_filename)\n",
        "      img = image.load_img(img_path, target_size=model.input_shape[1:3])\n",
        "      x = image.img_to_array(img)\n",
        "      x = np.expand_dims(x, axis=0)\n",
        "      x = preprocess_input(x)\n",
        "\n",
        "      # Use the pre-trained model to make predictions\n",
        "      predictions = model.predict(x, verbose=0)\n",
        "\n",
        "      # Decode and append the predicted classes\n",
        "      decoded_predictions = decode_predictions(predictions, top=1)[0][0]\n",
        "      if decoded_predictions[1] in ClassModel[ClassModelIndex] :\n",
        "          GoodPred += 1\n",
        "          averagePred.append(decoded_predictions[2])\n",
        "      else :\n",
        "          # Update the misclassifications dictionary\n",
        "          if decoded_predictions[1] not in misclassifications:\n",
        "              misclassifications[decoded_predictions[1]] = 1\n",
        "          else:\n",
        "              misclassifications[decoded_predictions[1]] += 1\n",
        "\n",
        "      NbimgClass += 1\n",
        "\n",
        "    accuracy = (GoodPred / NbimgClass) * 100\n",
        "    most_misclassified = max(misclassifications, key=misclassifications.get)\n",
        "    print(f\"For the class {class_dir}, the accuracy is {accuracy:.2f}%\")\n",
        "    print(f\"The mean probability score for the correct predictions is {np.mean(averagePred)*100} %\")\n",
        "    print(f\"The most frequently confused animal is {most_misclassified}\")\n",
        "    print(\"\")\n",
        "    ClassModelIndex += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KX3cInmWEsl"
      },
      "source": [
        "## Transfert learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgSntULAo2jQ"
      },
      "source": [
        "https://keras.io/api/applications/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo5ycRki-Y0I"
      },
      "source": [
        "VGG16 transfer learning (sans regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3730Yty-deK"
      },
      "outputs": [],
      "source": [
        "def TLModelArchi_1(ModelName, NUM_CLASS=1) :\n",
        "\n",
        "  IMG_SHAPE = (IMG_SIZE, IMG_SIZE, CHANEL)\n",
        "  base_model = ModelName(input_shape=IMG_SHAPE, include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  x = Dense(128, activation='relu')(x)\n",
        "  if NUM_CLASS <= 2 :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"sigmoid\")(x)\n",
        "  else :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "\n",
        "  model = tf.keras.Model(base_model.input, outputs)\n",
        "\n",
        "  model.compile(Lion(learning_rate=.0001), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9vdRgVruiS"
      },
      "source": [
        "VGG16 transfer learning (avec regularization et freeze des layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCEm6UNr0N-"
      },
      "outputs": [],
      "source": [
        "def TLModelArchi_2(ModelName, NUM_CLASS=1) :\n",
        "\n",
        "  base_model = ModelName(input_shape=IMG_SHAPE, include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Dense(512, activation='elu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  x = Dense(256, activation='elu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.25)(x)\n",
        "  if NUM_CLASS <= 2 :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"sigmoid\")(x)\n",
        "  else :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "\n",
        "  model = tf.keras.Model(base_model.input, outputs)\n",
        "\n",
        "  model.compile(Lion(learning_rate=.00001), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v13UACfcOOJt"
      },
      "source": [
        "VGG16 transfer learning (sans regularization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd4uFTsVNssh"
      },
      "outputs": [],
      "source": [
        "def TLModelArchi_2Trainable(ModelName, NUM_CLASS=1) :\n",
        "\n",
        "  base_model = ModelName(input_shape=IMG_SHAPE, include_top=False,\n",
        "                                                weights='imagenet')\n",
        "\n",
        "  base_model.trainable = True # By default is already True\n",
        "\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(512, activation='elu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(256, activation='elu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  if NUM_CLASS <= 2 :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"sigmoid\")(x)\n",
        "  else :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "  model = tf.keras.Model(base_model.input, outputs)\n",
        "\n",
        "  model.compile(Lion(learning_rate=.00001), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fO3DjX0KhRG"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEWravlKIGty"
      },
      "outputs": [],
      "source": [
        "def modelFine(modelName, NUM_CLASS=1) :\n",
        "\n",
        "  base_model = modelName(input_shape=IMG_SHAPE, include_top=False,\n",
        "                                                weights='imagenet')\n",
        "  base_model.trainable = False\n",
        "\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(512, activation='relu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = Dense(256, activation='relu', kernel_regularizer=L1L2(1e-3))(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  if NUM_CLASS <= 2 :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"sigmoid\")(x)\n",
        "  else :\n",
        "    outputs = layers.Dense(NUM_CLASS, activation=\"softmax\")(x)\n",
        "\n",
        "  model = tf.keras.Model(base_model.input, outputs)\n",
        "\n",
        "  base_learning_rate = 0.0001\n",
        "  model.compile(optimizer=Lion(learning_rate=base_learning_rate),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=METRICS)\n",
        "\n",
        "  return model\n",
        "\n",
        "def FineTuningKfoldingAugment(modelName, fine_layer_tune) :\n",
        "\n",
        "  histories_fine = []\n",
        "  epochs_training = []\n",
        "  scores = [[],[],[],[],[]]\n",
        "  kfold = KFold(N_KFOLDS, shuffle=True, random_state=SEED)\n",
        "  CALLBACKS = callbacks(str(modelName.__name__+'_fine'))\n",
        "\n",
        "  datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        shear_range=0.05,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "  for fold, (train_ix, eval_ix) in enumerate(kfold.split(X_train), start=1):  # Add enumerate to track the fold number\n",
        "\n",
        "    print(\"\")\n",
        "    print(f\"############# Fold n°{fold} #############\")\n",
        "\n",
        "    history_fine = []\n",
        "    model = modelFine(modelName)\n",
        "\n",
        "    split_point = int(len(train_ix) * 0.920)\n",
        "\n",
        "    # Combine eval_ix elements with train_ix for validation data\n",
        "    X_val = np.concatenate((X_train[train_ix[split_point:]], X_train[eval_ix]), axis=0)\n",
        "    y_val = np.concatenate((Y_train[train_ix[split_point:]], Y_train[eval_ix]), axis=0)\n",
        "\n",
        "    # Selection des données pour training\n",
        "    x_train, y_train = X_train[train_ix[:split_point]], Y_train[train_ix[:split_point]]\n",
        "\n",
        "    print(f\"len(y_train) : {len(y_train)} and len(y_val) : {len(y_val)}\")\n",
        "\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # 1 er training\n",
        "    history_fine.append(model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),\n",
        "                        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=CALLBACKS))\n",
        "\n",
        "    base_learning_rate = 0.0001\n",
        "\n",
        "    # Other training\n",
        "    for training in fine_layer_tune :\n",
        "      for layer in model.layers[training+1:] :\n",
        "        layer.trainable = True\n",
        "\n",
        "      model.compile(loss='binary_crossentropy',\n",
        "              optimizer = Lion(learning_rate=base_learning_rate/100),\n",
        "              metrics=METRICS)\n",
        "\n",
        "      epochs_training.append(history_fine[-1].epoch[-1]+1)\n",
        "\n",
        "      history_fine.append(model.fit(datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
        "                              epochs=EPOCHS, callbacks=CALLBACKS,\n",
        "                              validation_data=(X_val, y_val), batch_size=BATCH_SIZE,\n",
        "                              initial_epoch=history_fine[-1].epoch[-1]+1))\n",
        "\n",
        "    # Final history for this kfold\n",
        "    histories_fine.append(history_fine)\n",
        "\n",
        "    # evaluate du modele\n",
        "    loss, acc, pre, rec, fscore = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
        "\n",
        "    # Create a list of values from the evaluation\n",
        "    evaluation_values = [loss, acc, pre, rec, fscore]\n",
        "\n",
        "    # Append the values to the respective scores list\n",
        "    for i, value in enumerate(evaluation_values):\n",
        "        scores[i].append(value)\n",
        "\n",
        "  #plot acc and loss in function of epochs\n",
        "  plotCurvesFineTuningKfold(histories_fine)\n",
        "\n",
        "  ListMetrics = [\"loss\", \"accuracy\", \"precision\", \"recall\", \"f1-score\"]\n",
        "\n",
        "  for metric, ListMetricValue in zip(ListMetrics, scores) :\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean_accuracy = np.mean(ListMetricValue)\n",
        "    std_deviation = np.std(ListMetricValue)\n",
        "\n",
        "    # Calculate the standard error of the mean (SEM)\n",
        "    sem = std_deviation / np.sqrt(len(ListMetricValue))\n",
        "\n",
        "    # Set the desired confidence level (e.g., 95%)\n",
        "    confidence_level = 0.95\n",
        "\n",
        "    # Calculate the margin of error based on the confidence level\n",
        "    margin_of_error = stats.t.ppf((1 + confidence_level) / 2, len(ListMetricValue) - 1) * sem\n",
        "\n",
        "    # Calculate the confidence interval\n",
        "    lower_bound = mean_accuracy - margin_of_error\n",
        "    upper_bound = mean_accuracy + margin_of_error\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Test {metric}: {mean_accuracy:.3f} ± {margin_of_error:.3f} (95% CI: {lower_bound:.3f}, {upper_bound:.3f})\")\n",
        "\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO0rPpWc_ND4"
      },
      "source": [
        "## CNN + LSTM + Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwr_ZtGaApLJ"
      },
      "source": [
        "source :\n",
        "https://www.kaggle.com/code/siddarthapatnaik/2d-cnn-lstm-model-with-self-attention-mechanism#2D-CNN-Log-Mel-Spectrogram-without-Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-JSzSOxnfhD"
      },
      "outputs": [],
      "source": [
        "def get_2d_conv_LSTM_model(NUM_CLASS=1) :\n",
        "  model = Sequential()\n",
        "\n",
        "  # ---- Conv / Pool N°1\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°2\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°3\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # Reshape\n",
        "  model.add(Reshape((-1, 128)))\n",
        "\n",
        "  # LSTM\n",
        "  model.add(LSTM(128))\n",
        "\n",
        "  if NUM_CLASS <= 2 :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"sigmoid\"))\n",
        "  else :\n",
        "    model.add(layers.Dense(NUM_CLASS, activation=\"softmax\"))\n",
        "\n",
        "  # compilation du model de classification\n",
        "  opt = Lion(learning_rate=1e-04)\n",
        "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PizeZh7-nvY3"
      },
      "outputs": [],
      "source": [
        "def get_2d_conv_LSTM_atten_model() :\n",
        "  model = Sequential()\n",
        "\n",
        "  # ---- Conv / Pool N°1\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°2\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # ---- Conv / Pool N°3\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "\n",
        "  # Reshape\n",
        "  model.add(Reshape((-1, 128)))\n",
        "\n",
        "  #LSTM\n",
        "  model.add(LSTM(32, return_sequences=True))\n",
        "  model.add(SeqSelfAttention(attention_activation ='tanh'))\n",
        "  model.add(LSTM(32, return_sequences=False))\n",
        "\n",
        "  # Couche de sortie : classification => softmax pour nombreuse classes / sigmoid pour classe = 2\n",
        "  model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "  # compilation du model de classification\n",
        "  opt = Lion(learning_rate=1e-04)\n",
        "  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vit"
      ],
      "metadata": {
        "id": "KAjkCYw4x6IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 4\n",
        "num_epochs = 100\n",
        "input_shape = (256, 256, 3)\n",
        "image_size = 256  # We'll resize input images to this size\n",
        "patch_size = 6  # Size of the patches to be extract from the input images\n",
        "num_patches = 1024\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "vMTtppqb8xIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super().__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = Dense(units=projection_dim)\n",
        "        self.position_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "VBbvW0DF8Vit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "PW65vGb68RwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9mCi_33c8puK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(1)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    model.compile(Lion(learning_rate=.00001), loss='binary_crossentropy', metrics=METRICS)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "3Ogl8jdzx7kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZDJWo7cumO8"
      },
      "source": [
        "## Stacking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8v7ZM1UxV3q"
      },
      "outputs": [],
      "source": [
        "def stacked_model(ListPathModel, NUM_CLASS=1) :\n",
        "\n",
        "  loaded_models = []  # Create a dictionary to store the loaded models\n",
        "\n",
        "  for pathModel in ListPathModel :\n",
        "      model = load_model(pathModel)\n",
        "      model.trainable = False\n",
        "      loaded_models.append(model)\n",
        "\n",
        "  # Define the input layer\n",
        "  commonInput = tf.keras.Input(shape=(IMG_SIZE,IMG_SIZE,CHANEL))\n",
        "  out = []\n",
        "\n",
        "  # Iterate over the member models\n",
        "  for model in loaded_models:\n",
        "      # Rename each model's layers for distinction\n",
        "      model._name = model.get_layer(index=0)._name + \"-test\" + str(loaded_models.index(model) + 1)\n",
        "      # Pass the common input through each model\n",
        "      out.append(model(commonInput))\n",
        "\n",
        "  # Concatenate the outputs of the member models\n",
        "  modeltmp = tf.keras.layers.concatenate(out, axis=-1)\n",
        "  modeltmp = Dense(128, activation='relu')(modeltmp)\n",
        "  modeltmp = Dropout(0.2)(modeltmp)\n",
        "  modeltmp = Dense(64, activation='relu')(modeltmp)\n",
        "  modeltmp = Dropout(0.2)(modeltmp)\n",
        "\n",
        "  if NUM_CLASS <= 2 :\n",
        "    modeltmp = layers.Dense(NUM_CLASS, activation=\"sigmoid\")(features)\n",
        "  else :\n",
        "    modeltmp = layers.Dense(NUM_CLASS, activation=\"softmax\")(features)\n",
        "\n",
        "  # Create the stacked model with the concatenated outputs\n",
        "  stacked_model = Model(commonInput, modeltmp)\n",
        "  stacked_model.compile(Lion(learning_rate=.00001), loss='binary_crossentropy', metrics='accuracy')\n",
        "\n",
        "  return stacked_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNgUyG-77NIP"
      },
      "source": [
        "## Soup (ouai comme chez mamie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI8JKWlF7F1H"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import time\n",
        "\n",
        "def uniform_soup(model, path, by_name = False):\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except:\n",
        "        print(\"If you want to use 'Model Soup for Tensorflow2', please install 'tensorflow2'\")\n",
        "        return model\n",
        "\n",
        "    if not isinstance(path, list):\n",
        "        path = [path]\n",
        "    soups = []\n",
        "    for i, model_path in enumerate(path):\n",
        "        model.load_weights(model_path, by_name = by_name)\n",
        "        soup = [np.array(w) for w in model.weights]\n",
        "        soups.append(soup)\n",
        "    if 0 < len(soups):\n",
        "        for w1, w2 in zip(model.weights, list(zip(*soups))):\n",
        "            tf.keras.backend.set_value(w1, np.mean(w2, axis = 0))\n",
        "    return model\n",
        "\n",
        "def greedy_soup(model, path, data, metric, update_greedy = False, compare = np.greater_equal, by_name = False, digits = 4, verbose = True, y_true = \"y_true\"):\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "    except:\n",
        "        print(\"If you want to use 'Model Soup for Tensorflow2', please install 'tensorflow2'\")\n",
        "        return model\n",
        "\n",
        "    if not isinstance(path, list):\n",
        "        path = [path]\n",
        "    score, soup = None, []\n",
        "    input_key = [inp.name for inp in model.inputs]\n",
        "    input_cnt = len(input_key)\n",
        "    for i, model_path in enumerate(path):\n",
        "        if update_greedy:\n",
        "            model.load_weights(model_path, by_name = by_name)\n",
        "            for w1, w2 in zip(model.weights, soup):\n",
        "                tf.keras.backend.set_value(w1, np.mean([w1, w2], axis = 0))\n",
        "        else:\n",
        "            model = uniform_soup(model, soup + [model_path], by_name = by_name)\n",
        "\n",
        "        iterator = iter(data)\n",
        "        history = []\n",
        "        step = 0\n",
        "        start_time = time.time()\n",
        "        while True:\n",
        "            try:\n",
        "                text = \"\"\n",
        "                iter_data = next(iterator)\n",
        "                if not isinstance(iter_data, dict):\n",
        "                    x = iter_data[:input_cnt]\n",
        "                    y = list(iter_data[input_cnt:])\n",
        "                    d_cnt = len(y[0])\n",
        "                else:\n",
        "                    x = [iter_data[k] for k in input_key if k in iter_data]\n",
        "                step += 1\n",
        "                #del x\n",
        "\n",
        "                logits = model.predict(x)\n",
        "                if not isinstance(logits, list):\n",
        "                    logits = [logits]\n",
        "                if isinstance(iter_data, dict):\n",
        "                    metric_key = [key for key in inspect.getfullargspec(metric).args if key != \"self\"]\n",
        "                    if len(metric_key) == 0:\n",
        "                        metric_key = [y_true]\n",
        "                    y = [iter_data[k] for k in metric_key if k in iter_data]\n",
        "                    d_cnt = len(y[0])\n",
        "                metric_val = np.array(metric(*(y + logits)))\n",
        "                if np.ndim(metric_val) == 0:\n",
        "                    metric_val = [float(metric_val)] * d_cnt\n",
        "                history += list(metric_val)\n",
        "                #del y, logits\n",
        "\n",
        "                if verbose:\n",
        "                    sys.stdout.write(\"\\r[{name}] step: {step} - time: {time:.2f}s - {key}: {val:.{digits}f}\".format(name = os.path.basename(model_path), step = step, time = (time.time() - start_time), key = metric.__name__ if hasattr(metric, \"__name__\") else str(metric), val = np.nanmean(history), digits = digits))\n",
        "                    sys.stdout.flush()\n",
        "            except (tf.errors.OutOfRangeError, StopIteration):\n",
        "                print(\"\")\n",
        "                #gc.collect()\n",
        "                break\n",
        "        if 0 < len(history) and (score is None or compare(np.nanmean(history), score)):\n",
        "            score = np.nanmean(history)\n",
        "            if update_greedy:\n",
        "                soup = [np.array(w) for w in model.weights]\n",
        "            else:\n",
        "                soup += [model_path]\n",
        "    if len(soup) != 0:\n",
        "        if update_greedy:\n",
        "            for w1, w2 in zip(model.weights, soup):\n",
        "                tf.keras.backend.set_value(w1, w2)\n",
        "        else:\n",
        "            model = uniform_soup(model, soup, by_name = by_name)\n",
        "        if verbose:\n",
        "            print(\"greedy soup best score : {val:.{digits}f}\".format(val = score, digits = digits))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b30zPGj-p9F0"
      },
      "source": [
        "# Entrainement des modèles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9_70PLgD8Mx"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pql0gkF1-VW"
      },
      "outputs": [],
      "source": [
        "my_path=\"Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X, y = create_dataset(my_path, my_classes)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # .15 pour kfold et .3 pour normal\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=SEED)\n",
        "print(f\"Taille de X_train : {len(X_train)}, Taille de X_val : {len(X_val)}, Taille de X_test : {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIQ6vJZ7Qh-G"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyAugment(baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Y8LwOUko7X"
      },
      "outputs": [],
      "source": [
        "my_path=\"Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X, y = create_dataset(my_path, my_classes)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=SEED) # .15 pour kfold et .3 pour normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYDJyEeoD-O1"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq9f8yw2F6Sj"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(baselineWithRegularization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Yo5yxwQxBi"
      },
      "source": [
        "LSTM + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iypaxHqshS_h"
      },
      "outputs": [],
      "source": [
        "#trainModelNumpyKfoldAugment(get_2d_conv_LSTM_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lL51-1lhVNH"
      },
      "outputs": [],
      "source": [
        "#trainModelNumpyKfoldAugment(get_2d_conv_LSTM_atten_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbPoSDLkyKqi"
      },
      "source": [
        "## Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEDtpdt93Ka_"
      },
      "source": [
        "Testing pretrained model without use transfert learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWuizPLC3HQy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "TestingAlreadyTrainedModel(VGG16) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JdesMcgtegM"
      },
      "source": [
        "Transfer learning (check model here : https://keras.io/api/applications/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNr-gjxLyduU"
      },
      "source": [
        "Testing the first architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyQDvfDFx0A0",
        "outputId": "94c5f4b8-d131-44dc-eff9-fd3d625a7a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les classes :  ['tiger', 'Tiger_negative_class']\n",
            "Nombres de données :  200\n",
            "Résolution des images :  (256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "my_path=\"Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X, y = create_dataset(my_path, my_classes)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=SEED) # .15 pour kfold et .3 pour normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ0Q-F9ajvwV"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(TLModelArchi_1, VGG16) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_N3GuKsyhH3"
      },
      "source": [
        "Testing the second architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJeOrewu678k"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(TLModelArchi_2, VGG16) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDcjerlTy2mm"
      },
      "source": [
        "Testing with all model trainable parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LMRZKNRN5QV"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(TLModelArchi_2Trainable, VGG16) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE8rLQUSzT0i"
      },
      "source": [
        "VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rZH6otqtarl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
        "TestingAlreadyTrainedModel(VGG19) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0xEJvMD4e0_"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(TLModelArchi_2, VGG19) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEORfjf5zbU8"
      },
      "source": [
        "ResNet50v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl55ZSaCzVK5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
        "TestingAlreadyTrainedModel(ResNet50V2) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_zeD_DDtaj4"
      },
      "outputs": [],
      "source": [
        "trainModelNumpyKfoldAugment(TLModelArchi_2, ResNet50V2) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oSG4EWK0WKo"
      },
      "source": [
        "EfficientNetV2L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i1OmTzdmhVi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetV2L\n",
        "trainModelNumpyKfoldAugment(TLModelArchi_2, EfficientNetV2L) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKU-KmDE0Vyh"
      },
      "source": [
        "InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sb7NQ7atafA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import InceptionV3 # Scale input pixels between -1 and 1.\n",
        "X = tf.keras.applications.mobilenet.preprocess_input(X)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=SEED) # .15 pour kfold et .3 pour normal\n",
        "trainModelNumpyKfoldAugment(TLModelArchi_2, InceptionV3) # Add/Change the model Name for changing the pretaining model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vit_keras.vit import vit_l32\n",
        "model=create_vit_classifier()\n",
        "model.fit(X_train,Y_train,epochs=250, validation_data=(X_val,Y_val))"
      ],
      "metadata": {
        "id": "T3N8tueI1lLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SmGUOXAtl5z"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lg4TdDlF1Zqo"
      },
      "outputs": [],
      "source": [
        "my_path=\"Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X, y = create_dataset(my_path, my_classes)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.15, random_state=SEED) # .15 pour kfold et .3 pour normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3P7ZHgo_uEfk"
      },
      "outputs": [],
      "source": [
        "model = VGG16(include_top=False)\n",
        "\n",
        "n=1\n",
        "for layer in model.layers :\n",
        "  print(layer.name, n)\n",
        "  n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V9Wuq60j3wa"
      },
      "outputs": [],
      "source": [
        "fine_layer_tune=[17, 15, 11] # Définit selon les blocks du model\n",
        "FineTuningKfoldingAugment(VGG16, fine_layer_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9HjfhkTxzDx"
      },
      "outputs": [],
      "source": [
        "model = EfficientNetV2L(include_top=False)\n",
        "\n",
        "n=1\n",
        "for layer in model.layers :\n",
        "  print(layer.name, n)\n",
        "  n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc7bsLCGtpg1"
      },
      "outputs": [],
      "source": [
        "fine_layer_tune=[981,951,923,848,728]\n",
        "FineTuningKfoldingAugment(EfficientNetV2L, fine_layer_tune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqghzDdWxymK"
      },
      "outputs": [],
      "source": [
        "model = ResNet50V2(include_top=False)\n",
        "\n",
        "n=1\n",
        "for layer in model.layers :\n",
        "  print(layer.name, n)\n",
        "  n+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtP1JcOotpjQ"
      },
      "outputs": [],
      "source": [
        "fine_layer_tune=[178,167,155,87]\n",
        "FineTuningKfoldingAugment(ResNet50V2, fine_layer_tune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPUWIsB5tg7Q"
      },
      "source": [
        "## Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FqrtGcbbZvM"
      },
      "outputs": [],
      "source": [
        "my_path=\"Tiger-Fox-Elephant/\"\n",
        "my_classes=['tiger','Tiger_negative_class']\n",
        "X, y = create_dataset(my_path, my_classes)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=SEED) # .15 pour kfold et .3 pour normal\n",
        "X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, random_state=SEED)\n",
        "print(f\"Taille de X_train : {len(X_train)}, Taille de X_val : {len(X_val)}, Taille de X_test : {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8Ggk_yRqWTM"
      },
      "outputs": [],
      "source": [
        "ListModel=[\"Saved_Model/TLModelArchi_2VGG16.h5\",\"Saved_Model/TLModelArchi_2EfficientNetV2L.h5\", \"Saved_Model/TLModelArchi_2ResNet50V2.h5\"]\n",
        "trainModelNumpyAugment(stacked_model(ListModel))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2EbSJ_0DGdYH",
        "vaeN26nLNAJo",
        "ocC1yn8CKaz6",
        "RgJXlCHq3r72",
        "-fO3DjX0KhRG",
        "pO0rPpWc_ND4",
        "YZDJWo7cumO8",
        "mNgUyG-77NIP",
        "x9_70PLgD8Mx",
        "9SmGUOXAtl5z",
        "bPUWIsB5tg7Q"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}